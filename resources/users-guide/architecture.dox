/**
   @page architecture Introduction to the design of the dataclasses.

   @author Tyce DeYoung (deyoung@umdgrb.umd.edu)
   
   @section framework IceTray and the data classes

   The IceCube software execution framework is called IceTray.
   IceTray is responsible for seeing that data is passed around to
   various self-contained pieces of software, called modules.  IceTray
   is also responsible for seeing that these modules are called upon
   to execute at the appropriate times, for configuring modules
   properly based on a user-supplied configuration file or something
   similar, for handling logging and errors for the modules, and so
   forth.  

   The unit of data that IceTray passes around is called a Frame.
   IceTray knows nothing about the frame except what stream generated
   it -- it is only responsible for passing the frame around correctly.
   More information about frames can be found at @ref streams.

   The data classes are the things that can be put into the frame.
   They know nothing about modules or IceTray or execution: they are
   simply containers of information.  They are the language that
   IceTray modules use to communicate with each other.  

   @section streams Data Streams, Frames, and Database Records

   These classes, as well as IceTray, were designed to support the
   Frame/Stream/Stop
   data model, which was adopted by IceCube.  This model separates
   the data flow into several 'streams,' based on the rate at which the
   data in that stream changes.  For example, IceCube will have a
   Geometry stream,
   with new records coming only at a glacial pace; a Physics stream
   that records the event-by-event output of the DAQ; and several
   streams on intermediate time scales such as OM Status, Monitoring,
   Time Calibration, and so forth.

   The data in any one of these streams make sense only in the context
   of all the others, so data is provided to applications in the form
   of 'frames,' which contain the most recent record from each
   stream.  Because most applications are interested only in new
   records from a single stream (usually the Physics stream),
   application modules register their interest in a stream with the
   IceTray framework and are provided only those frames which were
   generated as a result of  a new record (a 'stop') in that stream.

   The data in most streams (other than Physics) is stored centrally
   in the IceCube database.  However, in some instances it is
   convenient to copy the relevant database records into the data
   file, which eliminates the need for network access.  The data
   classes are therefore organized in a header/contents fashion: each
   stream must contribute at least a header object to each frame.
   This header contains the information necessary to make a database
   call.  The frame may also contain an optional contents object
   containing the full  information.  If the full record is not
   present, a special-purpose IceTray module can make a database query
   to obtain the record and add the contents object before the frame
   is passed to processing or analysis modules.  At output time, the
   user can decide whether the full information is written to the
   file, or just the headers.

   @section extension Evolution and Extension
   
   The data classes must be flexible enough to accommodate continued
   development as software and analysis techniques evolve.  At the same
   time, there must be a well-defined and universally accepted structure
   to the data, so that different programmers share a common
   understanding of how various types of information are to be stored and
   users can know how to find the information they need for their analyses.
   
   These data classes were drawn up, based on experience from AMANDA and
   on the design of IceCube, to provide both a standard data structure
   and well-defined methods for extending that structure with minimal
   disruption.  For "standard" types of data -- those which come from
   authoritative sources, such as geometry records or DAQ output -- these
   classes will be used directly in the event record.  Such classes will
   typically be modified only rarely, and not by the average developer.
   Root's automatic schema evolution will be used to accommodate such
   evolution without user intervention.
   
   There are also data types which are not standardized, which are
   produced by a variety of different algorithms and whose content is not
   fixed.  Records such as reconstruction results, trigger and filter
   records, feature extraction or hit reconstruction results, and OM
   selections fall in this category.   For these data, the classes
   provided are intended to serve as base classes which specific
   algorithms can use to provide core functionality while extending the
   classes through inheritance.  
   
   These classes thus specify a minimal interface for particular data
   types and provide an implementation of that basic functionality, but
   the developer should derive from these classes as needed.
   For example, the basic @c RecoHit class includes a @c Time record, and
   anyone developing feature extraction algorithms should use the @c
   RecoHit class and provide times for the hits they extract.  If the
   developer also wishes to record an uncertainty on the time, he or she
   should define a new class inheriting from the basic @c RecoHit class
   and adding the new features:
   @code
class NewRecoHit : public RecoHit {
private:
    double fUncert;
public:
    double TimeUncert() {return fUncert;};
};
   @endcode
   With this method of extending classes, algorithms that are prepared to
   make use of the extra information can access it, while algorithms that
   do not make use of
   the new features can simply use the base class interface without
   knowing anything about the details of the specific algorithm or its
   special parameters.  This general approach is used throughout the
   data classes.  The
   goal is to make it as easy as possible to develop new algorithms
   and incorporate those algorithms
   into the official production software, while minimizing the extent to
   which existing code must be modified to adapt to such changes.  

   We have not attempted to make the system foolproof, however;
   we assume that users setting up processing or analysis chain have
   at least a rudimentary understanding of the pieces of code they are
   using.  For example, if you use a reconstruction algorithm that relies on a
   particular type of hit extraction result, you must
   ensure that your processing chain includes the module that provides
   that result, or your code will fail.  For developers, this means
   that it is important to document your software, making clear what
   requirements it has and what data it produces.  A list of best
   practices that should be followed in your code 
<!-- can be found at @ref best-practices.
-->
   is being produced.
 
   @section containers Lists of objects
 
   Many items in the data structure consist of lists of other items:
   lists of hits, lists of particles, lists of OMs, and so forth.  In
   many cases the nature of the elements in a list cannot be
   predicted: will the particles be tracks or showers?  In
   some cases the elements in a list may be heterogeneous: some of the
   OMs are IceCube OMs and some are AMANDA OMs.  This presents some
   technical problems, not least because Root and 'standard' C++
   solve these problems differently.

   In general we have adopted an STL-style syntax for collections of
   objects.  The actual
   implementation of these 'container' classes is completely modular,
   however, with a global setting that can be easily changed to
   replace STL containers with Root containers
   or any other implementation.  This will allow us to optimize the
   implementation without affecting existing code.

   There are two basic types of containers found in the data classes.
   Following STL nomenclature these are called 'maps' and 'vectors.'
   Vectors are simply ordered lists, like C arrays.  Each element in a
   vector has a unique index number.  Vectors should be used where
   the items are naturally identified by number, like lists of OMs or
   hits.  

   Maps are like vectors, except
   that each element is identified by name rather than by number.  The
   syntax of addressing a particular element is the same as for
   vectors, except that the index "number" of a map is actually a
   string, for example
   <code>recoResult["muonReco"]</code>.   Maps should be used where
   the items have some more general identity, as with the results of
   reconstruction algorithms (@c "linefit", @c "JAMS", etc.) or the
   particular solutions found by a reconstruction 
   (@c "best-upgoing"). 

   STL provides basic methods
   for doing things like iterating over each element in a list, adding
   elements to a list, and so forth, which also incorporate basic
   error checking.  Developers are encouraged to use these features in
   their code.  A short tutorial is available at @ref stlsyntax.

   @section pointers Pointers and references to data

   In general much of the data relies on context: a reconstruction may
   store information about what hits were used in the fit, but those
   hits are not a part of the reconstruction, because they may also be
   used in other fits.  The hits are not useful without the geometry
   records corresponding to those OMs, but the geometry is stored
   separately because it is part of a different stream, and so forth.
   We have decided not to include pointers between such related types
   of data, because the number of pointers required would be extremely
   large, because it is not trivial to ensure that such pointers
   are always initialized and maintained, and because such pointers
   raise many issues related to ownership and copying.  Instead, we
   rely on index numbers or names to store such information: a hit
   records that it occurred in OM number 4126, a reconstruction
   records that the track used for its first guess was the track named
   @c "lineFit".  There is, in general, a single path from the frame
   down to any particular piece of data, and the overall layout of the
   event is standardized (see @ref layout).  A software module can
   then traverse the data structure to find whatever data it requires.

   Pointers are used in the code, of course.  They are used where the
   object is expected to be derived from a standard base class, or in
   heterogeneous containers of data.  But in general an object will be
   pointed to by only one object, so that ownership is clear and it is
   easy to ensure that pointers are properly initialized.

   Pointers to each class are typedef'd, for example @c TObjectPtr.
   Developers are strongly encouraged to use these typedefs.  These
   will allow us to globally introduce smart pointers if necessary;
   see @ref memorymanagement for more information.

   @section root A note regarding Root
   
   It is expected that most users will use Root for physics analysis, and
   these classes have been designed to work well in interactive Root
   sessions.  Root is also used extensively for I/O in the system, and is
   the basis for applications like the event viewer that use these
   classes.  However, Root classes (other than @c TObject) are not
   explicitly used in the class definitions.  This will allow us to
   solve in a centralized manner any interface problems that arise as
   Root and the IceCube software evolve.  

*/
